{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Хакатон Моторика\n",
    "\n",
    "## Часть 4 Анализ данных и разрабоотка свертки \n",
    "\n",
    "Для улучшения качества работы наших протезов мы решаем задачу “распознавания жестов”. \n",
    "\n",
    "Задача: построение классификационной модели, которая по показаниям от датчиков будет определять жест, выполненный пользователем протеза во время записи данных, что отражено в значении целевой переменной.\n",
    "\n",
    "Данные: это оптомиография, сырые значения оптической плотности (optical density) + шумы и дрейфы. Оптическая плотность может изменяться в зависимости от выполняемого жеста. Измеряется в значениях АЦП, это отчёты от 0 до 4096, в диапазоне от 0 до 3.3в. Частота измерения - 30 Гц. \n",
    "\n",
    "В предыдущих частях было рассмотрено создание MVP, создание малой модели на основе площади ступеньки. В этой части я хочу дорабоать малую модель. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import time\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165, 40, 60)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('data/X_train.npy')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('data/y_train.csv', index_col=0)\n",
    "y = y.Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция случацного добавления шума к временному ряду наблюдений с датчика\n",
    "def add_gauss_noise(orig, level=3., prob=0.5):\n",
    "    size = orig.shape[0]\n",
    "    cond = np.where(np.random.rand(size)<=prob,1,0)\n",
    "    addn = np.random.normal(loc=0.0, scale = level, size = size )\n",
    "    return orig + cond * addn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция расширения набора данных за счет добавления шума к данным\n",
    "def expand_data(X_orig, y_orig, qty=165, level=3., prob=0.5):\n",
    "    orig_qty = X_orig.shape[0] - 1\n",
    "    orig_chan = X_orig.shape[1]\n",
    "    orig_time = X_orig.shape[2]\n",
    "    X_ext = X_orig ; y_ext = y_orig\n",
    "    for i in range(qty):\n",
    "        loc = round(np.random.rand()* orig_qty)\n",
    "        y_ext = pd.concat([y_ext,pd.Series(np.array([y_orig[loc]]), name='Class') ], axis=0, ignore_index=True)\n",
    "        x_add = np.zeros((orig_chan,orig_time))\n",
    "        for j in range(orig_chan):\n",
    "            x_add[j,:] = add_gauss_noise(X_orig[loc,j,:], level=level, prob=prob)    #.reshape(-1),spv).reshape(1,-1)\n",
    "        #x_add = x_add.reshape(1,orig_chan,orig_time)\n",
    "        X_ext = np.concatenate((X_ext,x_add.reshape(1,orig_chan,orig_time)),axis=0)\n",
    "    return X_ext, y_ext \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширяю набор данных за счет дублирования старых с небольшими случайными изменениями. Идея решения позаимствована из компьютерного зрения, задачи аугментации исходных изображений.  Изменение происходит за счет добавления гауссова шума с СКО 3 и вероятностью изменения 0.5.  Уровень 3 определен в части 2 как среднее СКО уровня фонового шума с сенсоров. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 40, 60) (330,)\n"
     ]
    }
   ],
   "source": [
    "X_exp, y_exp = expand_data(data,y)\n",
    "print(X_exp.shape, y_exp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_low_sens(data, level=200.):\n",
    "    sens2del = list()\n",
    "    for i in range(data.shape[1]):\n",
    "        if data[:,i,:].max() < level:  sens2del.append(i)\n",
    "    return sens2del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_low_sens(data, sens_list):\n",
    "    return np.delete(data, sens_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нахожу а затем удаляю сигналы с сенсоров, которые имеют уровень меньше 200. Уровень 200 получен в предыдущей части в работе другого участника команды. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sens to delete [1, 3, 4, 6, 7, 9, 10, 11, 13, 14, 16, 18, 20, 22, 23, 25, 26, 28, 31, 32, 34, 35, 37, 39]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(330, 16, 60)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_sens_nums = find_low_sens(X_exp, level=200.)\n",
    "print('Sens to delete', low_sens_nums)\n",
    "X_cleared = delete_low_sens(X_exp, low_sens_nums)\n",
    "X_cleared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтрация первой производной по амлитуде (больше порога) и последующее интегрирование\n",
    "# data - оригинальные измерения с датчиков\n",
    "# thr - пороговый уровень фильтрации первой производной\n",
    "def cum_signed_diff(data, thr): \n",
    "    # t_frame = data.shape[0]\n",
    "    n_sens = data.shape[0]\n",
    "    cdiff = np.zeros(n_sens)\n",
    "    for s in range(n_sens):\n",
    "        ddata = np.diff(data[s,:])\n",
    "        fddata = np.where(np.abs(ddata)<thr,0,ddata)\n",
    "        cdiff[s] = fddata.sum()\n",
    "    return cdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция расчета площади шага кривой \n",
    "# Сигнал с датчика может пойти вверх или вниз или подпрыгнуть на месте\n",
    "def parse_data(data, thr):\n",
    "    n_samples = data.shape[0]\n",
    "    n_sens = data.shape[1]\n",
    "    p_data = np.zeros((n_samples,n_sens))  # Я предполагаю сократить размерность задачи на одно измерение. \n",
    "    for i in range(n_samples):\n",
    "        p_data[i] = cum_signed_diff(data[i,:,:], thr)\n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбираю сигнал каждого датчика - беру первую производную, фильтрую абсолютное занчение амлитуды по уровню thr = 10 (все что меньше = 0), затем рассчитываю кумулятивную сумму отфильтрованных значений первой производной - сигнал шага уровня плотности вверх или вниз.  Уровень thr = 10  определен в части 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обработанного массива значений шагов (330, 16)\n",
      "[ -424. -1780. -1479.  -451.    32.   -15.   170.   154.    26.   228.\n",
      "   -26.    61.    32.    13.   192.    72.]\n"
     ]
    }
   ],
   "source": [
    "X_parsed = parse_data(X_cleared, 10.)\n",
    "print('Размер обработанного массива значений шагов', X_parsed.shape)\n",
    "print(X_parsed[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 898  1670  1482  715  674  496  396  1182  378  682  1252  907  2067  148  455  550  \n",
      "Min: -1177  -1780  -1479  -775  -730  -433  -313  -932  -236  -638  -1068  -698  -2120  -141  -342  -475  "
     ]
    }
   ],
   "source": [
    "print('Max: ', end='')\n",
    "for i in range(X_parsed.shape[1]):\n",
    "    print(round(X_parsed[:,i].max()), ' ', end='')\n",
    "\n",
    "print('\\nMin: ', end='')\n",
    "for i in range(X_parsed.shape[1]):\n",
    "    print(round(X_parsed[:,i].min()), ' ', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_norm_factor(data):\n",
    "    n_sens = data.shape[1]\n",
    "    sens_fact = np.zeros((n_sens))\n",
    "    for i in range(n_sens):\n",
    "        sens_fact[i] = np.abs(data[:,i]).max()\n",
    "    return sens_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_sens(data, factor):\n",
    "    n_sens = data.shape[1]\n",
    "    for i in range(n_sens):\n",
    "        data[:,i] = data[:,i] / factor[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормирую амлитуду суммарной отфильтрованной производной в каждом канале. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 0.763  0.938  1.0  0.923  0.923  1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.975  1.0  1.0  1.0  \n",
      "Min: -1.0  -1.0  -0.998  -1.0  -1.0  -0.873  -0.789  -0.788  -0.625  -0.936  -0.853  -0.77  -1.0  -0.952  -0.752  -0.864  "
     ]
    }
   ],
   "source": [
    "norm_factor = find_norm_factor(X_parsed)\n",
    "X_norm = normalise_sens(X_parsed,norm_factor)\n",
    "print('Max: ', end='')\n",
    "for i in range(X_norm.shape[1]):\n",
    "    print(round(X_norm[:,i].max(),3), ' ', end='')\n",
    "\n",
    "print('\\nMin: ', end='')\n",
    "for i in range(X_norm.shape[1]):\n",
    "    print(round(X_norm[:,i].min(),3), ' ', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.36023789 -1.         -0.99797571 -0.58193548  0.04382144 -0.03024194\n",
      "  0.42929293  0.13028765  0.06878307  0.33424596 -0.02076677  0.06727855\n",
      "  0.01509434  0.08793876  0.42197802  0.13094305]\n"
     ]
    }
   ],
   "source": [
    "print(X_norm[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаю полиномиальные признаки второго порядка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.60237893e-01 -1.00000000e+00 -9.97975709e-01 -5.81935484e-01\n",
      "  4.38214444e-02 -3.02419355e-02  4.29292929e-01  1.30287648e-01\n",
      "  6.87830688e-02  3.34245959e-01 -2.07667732e-02  6.72785496e-02\n",
      "  1.50943396e-02  8.79387617e-02  4.21978022e-01  1.30943052e-01\n",
      "  1.29771340e-01  3.60237893e-01  3.59508666e-01  2.09635213e-01\n",
      " -1.57861448e-02  1.08942911e-02 -1.54647580e-01 -4.69345478e-02\n",
      " -2.47782678e-02 -1.20408060e-01  7.48097861e-03 -2.42362829e-02\n",
      " -5.43755310e-03 -3.16788742e-02 -1.52012474e-01 -4.71706493e-02\n",
      "  1.00000000e+00  9.97975709e-01  5.81935484e-01 -4.38214444e-02\n",
      "  3.02419355e-02 -4.29292929e-01 -1.30287648e-01 -6.87830688e-02\n",
      " -3.34245959e-01  2.07667732e-02 -6.72785496e-02 -1.50943396e-02\n",
      " -8.79387617e-02 -4.21978022e-01 -1.30943052e-01  9.95955515e-01\n",
      "  5.80757477e-01 -4.37327370e-02  3.01807170e-02 -4.28423915e-01\n",
      " -1.30023908e-01 -6.86438318e-02 -3.33569347e-01  2.07247352e-02\n",
      " -6.71423582e-02 -1.50637843e-02 -8.77607480e-02 -4.21123815e-01\n",
      " -1.30677985e-01  3.38648907e-01 -2.55012535e-02  1.75988554e-02\n",
      " -2.49820789e-01 -7.58190055e-02 -4.00273084e-02 -1.94509584e-01\n",
      "  1.20849222e-02 -3.91517753e-02 -8.78393183e-03 -5.11746858e-02\n",
      " -2.45563984e-01 -7.62004085e-02  1.92031899e-03 -1.32524529e-03\n",
      "  1.88122362e-02  5.70939293e-03  3.01417343e-03  1.46471407e-02\n",
      " -9.10029996e-04  2.94824322e-03  6.61455765e-04  3.85360356e-03\n",
      "  1.84916864e-02  5.73811369e-03  9.14574662e-04 -1.29826491e-02\n",
      " -3.94015065e-03 -2.08013313e-03 -1.01082447e-02  6.28027414e-04\n",
      " -2.03463356e-03 -4.56482045e-04 -2.65943836e-03 -1.27614321e-02\n",
      " -3.95997134e-03  1.84292419e-01  5.59315661e-02  2.95280851e-02\n",
      "  1.43489427e-01 -8.91502888e-03  2.88822056e-02  6.47989327e-03\n",
      "  3.77514886e-02  1.81152181e-01  5.62129265e-02  1.69748712e-02\n",
      "  8.96158426e-03  4.35481198e-02 -2.70565403e-03  8.76556399e-03\n",
      "  1.96660601e-03  1.14573344e-02  5.49785240e-02  1.70602623e-02\n",
      "  4.73111055e-03  2.29904628e-02 -1.42840239e-03  4.62762510e-03\n",
      "  1.03823500e-03  6.04869789e-03  2.90249433e-02  9.00666497e-03\n",
      "  1.11720361e-01 -6.94121000e-03  2.24875833e-02  5.04522202e-03\n",
      "  2.93931757e-02  1.41044448e-01  4.37671860e-02  4.31258868e-04\n",
      " -1.39715838e-03 -3.13460727e-04 -1.82620432e-03 -8.76312186e-03\n",
      " -2.71926466e-03  4.52640324e-03  1.01552528e-03  5.91639234e-03\n",
      "  2.83900693e-02  8.80965864e-03  2.27839089e-04  1.32737753e-03\n",
      "  6.36947958e-03  1.97649890e-03  7.73322581e-03  3.71082247e-02\n",
      "  1.15149699e-02  1.78065451e-01  5.52550902e-02  1.71460829e-02]\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_norm)\n",
    "X_poly.shape\n",
    "print(X_poly[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: (66, 152)\n"
     ]
    }
   ],
   "source": [
    "# Разделение данных на обучение и валидацию\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y_exp, test_size = 0.2)\n",
    "print('Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробую логистическую мультиклассовую классификацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, max_iter=5000, multi_class='multinomial',\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=10, random_state=42, max_iter=5000)\n",
    "LR_clf.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = clf.predict(X_test_review_tfidf)\n",
    "y_pred = LR_clf.predict(X_test)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "LR_clf.fit(X_poly,y_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробую собрать файл для scoreboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (166, 40, 60)\n",
      "Deleted: (166, 16, 60)\n",
      "Stepped (166, 16)\n",
      "[ 255. 1081.  867.  140. -148.  -18. -199. -116.  -32.  -93.  -52. -151.\n",
      "   61.  -36. -203.  -62.]\n",
      "[ 0.21665251  0.60730337  0.58502024  0.18064516 -0.20267418 -0.03629032\n",
      " -0.50252525 -0.09813875 -0.08465608 -0.13633717 -0.04153355 -0.16654198\n",
      "  0.02877358 -0.24352272 -0.44615385 -0.11275652]\n",
      "Polynomized (166, 152)\n"
     ]
    }
   ],
   "source": [
    "X_score = np.load('data/X_test.npy')\n",
    "print('Loaded:', X_score.shape)\n",
    "X_score = delete_low_sens(X_score, low_sens_nums)\n",
    "print('Deleted:', X_score.shape)\n",
    "X_score = parse_data(X_score, 10.)\n",
    "print('Stepped', X_score.shape)\n",
    "print(X_score[0,:])\n",
    "X_score = normalise_sens(X_score,norm_factor)\n",
    "print(X_score[0,:])\n",
    "X_score = poly.transform(X_score)\n",
    "print('Polynomized', X_score.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = pd.Series(data=LR_clf.predict(X_score),index=np.arange(166)+165,name='Class')\n",
    "y_score.to_csv('220911_01.csv', index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Score: 0.98795\n",
    "\n",
    "Результат чуточку хуже идеального.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробую собрать обучающий набор из обучающих и тестовых данных и использовать ранее использованный результат для разметки данных.  Подход отчасти напомниает целевое кодирование.  Использование подхода обусловлено следующими соображениями:\n",
    "- есть хорошо распознанные данные, однако модель их распознвания не влезет в контроллер; \n",
    "- можно попробовать обучить модель меньшего размера на исходных данных и результате распознавания такой большой модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331, 40, 60)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_new = np.concatenate((np.load('data/X_train.npy'),np.load('data/X_test.npy')),axis=0)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = pd.read_csv('data/y_train.csv', index_col=0)\n",
    "y1 = y1.Class.values\n",
    "y2 = pd.read_csv('data/LR_submission121.csv', index_col=0)\n",
    "y2 = y2.Class.values\n",
    "y_new = np.concatenate((y1,y2),axis=0)\n",
    "y_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(662, 40, 60) (662,)\n"
     ]
    }
   ],
   "source": [
    "X_new, y_new = expand_data(X_new, pd.Series(y_new, name='Class'), qty=331)\n",
    "print(X_new.shape, y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X_new, low_sens_nums, norm_factor, fit=True):\n",
    "    if fit : low_sens_nums = find_low_sens(X_new, level=200.)\n",
    "    print('Sens to delete', low_sens_nums)\n",
    "    X_new = delete_low_sens(X_new, low_sens_nums)\n",
    "    print('Deleted:', X_new.shape)\n",
    "    X_new = parse_data(X_new, 10.)\n",
    "    print('Stepped', X_new.shape)\n",
    "    print(X_new[0,:])\n",
    "    if fit : norm_factor = find_norm_factor(X_new)\n",
    "    X_new = normalise_sens(X_new,norm_factor)\n",
    "    print(X_new[0,:])\n",
    "    X_new = poly.transform(X_new)\n",
    "    print('Polynomized', X_new.shape)\n",
    "    return X_new, low_sens_nums, norm_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sens to delete [1, 3, 4, 6, 7, 9, 10, 11, 13, 14, 16, 18, 20, 22, 23, 25, 26, 28, 31, 32, 34, 35, 37, 39]\n",
      "Deleted: (662, 16, 60)\n",
      "Stepped (662, 16)\n",
      "[ -424. -1780. -1479.  -451.    32.   -15.   170.   154.    26.   228.\n",
      "   -26.    61.    32.    13.   192.    72.]\n",
      "[-0.35743609 -0.98779134 -0.96976602 -0.58193548  0.04351242 -0.02914835\n",
      "  0.39945826  0.1165571   0.06878307  0.33137174 -0.02044325  0.06745177\n",
      "  0.01494629  0.0785088   0.42197802  0.1302006 ]\n",
      "Polynomized (662, 152)\n",
      "Test: (133, 152)\n",
      "Test Accuracy:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, max_iter=5000, multi_class='multinomial',\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# подготовка данных и запоминание констант обработки\n",
    "X_new, low_sens_nums, norm_factor = prepare_data(X_new, low_sens_nums, norm_factor, fit=True)\n",
    "# Разделение на обучение и валидацию \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size = 0.2)\n",
    "print('Test:', X_test.shape)\n",
    "LR_clf.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = clf.predict(X_test_review_tfidf)\n",
    "y_pred = LR_clf.predict(X_test)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "# Обучаю модель еще раз на всех данных\n",
    "LR_clf.fit(X_new,y_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sens to delete [1, 3, 4, 6, 7, 9, 10, 11, 13, 14, 16, 18, 20, 22, 23, 25, 26, 28, 31, 32, 34, 35, 37, 39]\n",
      "Deleted: (166, 16, 60)\n",
      "Stepped (166, 16)\n",
      "[ 255. 1081.  867.  140. -148.  -18. -199. -116.  -32.  -93.  -52. -151.\n",
      "   61.  -36. -203.  -62.]\n",
      "[ 0.21496746  0.59988901  0.56848353  0.18064516 -0.20124496 -0.03497802\n",
      " -0.46760114 -0.08779626 -0.08465608 -0.13516479 -0.04088651 -0.16697078\n",
      "  0.02849136 -0.21740899 -0.44615385 -0.11211718]\n",
      "Polynomized (166, 152)\n"
     ]
    }
   ],
   "source": [
    "X_score = np.load('data/X_test.npy')\n",
    "X_score, _a, _b = prepare_data(X_score, low_sens_nums, norm_factor, fit=False)\n",
    "y_score = pd.Series(data=LR_clf.predict(X_score),index=np.arange(166)+165,name='Class')\n",
    "y_score.to_csv('220912_01.csv', index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Score: 1.000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюме - относительно простая модель может быть дополнительно обучена на ранее не распознававшихся данных, размеченных результатами работы большей по размеру модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "addbcdd03704fafbc83443546c5ba4d9ee80e366948352667fd4709d4f794dc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
